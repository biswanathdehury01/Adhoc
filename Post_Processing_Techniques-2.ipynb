{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Dialation and Erosion"
      ],
      "metadata": {
        "id": "fGiRQWA1lGND"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "92-Kfl-lhUgg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "outputId": "68444f1e-d3aa-40e6-d49e-0a105e1350be"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "DisabledFunctionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mDisabledFunctionError\u001b[0m                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-dfda382a0d88>\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Display the results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Original Image'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Dilated Image'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdilated_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Eroded Image'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meroded_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_import_hooks/_cv2.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mDisabledFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mDisabledFunctionError\u001b[0m: cv2.imshow() is disabled in Colab, because it causes Jupyter sessions\nto crash; see https://github.com/jupyter/notebook/issues/3935.\nAs a substitution, consider using\n  from google.colab.patches import cv2_imshow\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_snippet",
                "actionText": "Search Snippets for cv2.imshow",
                "snippetFilter": "cv2.imshow"
              }
            ]
          }
        }
      ],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Load the image\n",
        "image = cv2.imread('/content/drive/MyDrive/images_folder/pngtree-car-black-daily-travel-trolley-sports-car-png-image_6561576.png.jpeg', 0)  # Read the image in grayscale\n",
        "\n",
        "# Define the structuring element\n",
        "kernel = np.ones((5, 5), np.uint8)  # 5x5 square structuring element\n",
        "\n",
        "# Perform dilation\n",
        "dilated_image = cv2.dilate(image, kernel, iterations=1)\n",
        "\n",
        "# Perform erosion\n",
        "eroded_image = cv2.erode(image, kernel, iterations=1)\n",
        "\n",
        "# Perform opening\n",
        "opened_image = cv2.morphologyEx(image, cv2.MORPH_OPEN, kernel)\n",
        "\n",
        "# Perform closing\n",
        "closed_image = cv2.morphologyEx(image, cv2.MORPH_CLOSE, kernel)\n",
        "\n",
        "# Perform morphological gradient\n",
        "gradient_image = cv2.morphologyEx(image, cv2.MORPH_GRADIENT, kernel)\n",
        "\n",
        "# Display the results\n",
        "cv2.imshow('Original Image', image)\n",
        "cv2.imshow('Dilated Image', dilated_image)\n",
        "cv2.imshow('Eroded Image', eroded_image)\n",
        "cv2.imshow('Opened Image', opened_image)\n",
        "cv2.imshow('Closed Image', closed_image)\n",
        "cv2.imshow('Gradient Image', gradient_image)\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KTmoRdlNlCEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Contour Refinment"
      ],
      "metadata": {
        "id": "kgJKMzVglCjk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Load the image\n",
        "image = cv2.imread('image.jpg')\n",
        "\n",
        "# Convert the image to grayscale\n",
        "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# Perform thresholding to obtain a binary image\n",
        "_, binary = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "# Find contours in the binary image\n",
        "contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "# Create an empty mask for contour refinement\n",
        "mask = np.zeros_like(gray)\n",
        "\n",
        "# Iterate over each contour\n",
        "for contour in contours:\n",
        "    # Perform contour approximation\n",
        "    epsilon = 0.03 * cv2.arcLength(contour, True)\n",
        "    approx = cv2.approxPolyDP(contour, epsilon, True)\n",
        "\n",
        "    # Draw the refined contour on the mask\n",
        "    cv2.drawContours(mask, [approx], 0, (255), thickness=cv2.FILLED)\n",
        "\n",
        "# Apply the mask to the original image\n",
        "refined_image = cv2.bitwise_and(image, image, mask=mask)\n",
        "\n",
        "# Display the results\n",
        "cv2.imshow('Original Image', image)\n",
        "cv2.imshow('Refined Image', refined_image)\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "FPzgARmzimyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xdPsGZATlLZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Load the image\n",
        "image = cv2.imread('image.jpg')\n",
        "\n",
        "# Gaussian Blur\n",
        "blurred_image = cv2.GaussianBlur(image, (5, 5), 0)\n",
        "\n",
        "# Median Filter\n",
        "filtered_image = cv2.medianBlur(image, 5)\n",
        "\n",
        "# Bilateral Filter\n",
        "bilateral_filtered_image = cv2.bilateralFilter(image, 9, 75, 75)\n",
        "\n",
        "# Non-local Means Denoising\n",
        "denoised_image = cv2.fastNlMeansDenoisingColored(image, None, 10, 10, 7, 21)\n",
        "\n",
        "# Additional Techniques\n",
        "box_filtered_image = cv2.boxFilter(image, -1, (5, 5))\n",
        "normalized_blurred_image = cv2.blur(image, (5, 5))\n",
        "morphological_filtered_image = cv2.morphologyEx(image, cv2.MORPH_CLOSE, np.ones((5, 5), np.uint8))\n",
        "\n",
        "# Display the results\n",
        "cv2.imshow('Original Image', image)\n",
        "cv2.imshow('Gaussian Blur', blurred_image)\n",
        "cv2.imshow('Median Filter', filtered_image)\n",
        "cv2.imshow('Bilateral Filter', bilateral_filtered_image)\n",
        "cv2.imshow('Non-local Means Denoising', denoised_image)\n",
        "cv2.imshow('Box Filter', box_filtered_image)\n",
        "cv2.imshow('Normalized Blur', normalized_blurred_image)\n",
        "cv2.imshow('Morphological Filter', morphological_filtered_image)\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        },
        "id": "RUlXqyPyimvc",
        "outputId": "748b3158-a719-48f9-e78e-1308a305aa6e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "error",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-e1ac261d9ddc>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Gaussian Blur\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mblurred_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGaussianBlur\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Median Filter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31merror\u001b[0m: OpenCV(4.7.0) /io/opencv/modules/imgproc/src/smooth.dispatch.cpp:617: error: (-215:Assertion failed) !_src.empty() in function 'GaussianBlur'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qsdN-7Ldimsd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from skimage import morphology\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the segmented mask\n",
        "mask = cv2.imread('segmented_mask.png')\n",
        "\n",
        "# Convert the mask to grayscale\n",
        "gray_mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# Perform erosion\n",
        "eroded_mask = morphology.erosion(gray_mask, selem=morphology.square(3))\n",
        "\n",
        "# Perform dilation\n",
        "dilated_mask = morphology.dilation(gray_mask, selem=morphology.square(5))\n",
        "\n",
        "# Display the original mask, eroded mask, and dilated mask\n",
        "fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n",
        "\n",
        "# Original mask\n",
        "axs[0].imshow(cv2.cvtColor(mask, cv2.COLOR_BGR2RGB))\n",
        "axs[0].set_title('Original Mask')\n",
        "\n",
        "# Eroded mask\n",
        "axs[1].imshow(eroded_mask, cmap='gray')\n",
        "axs[1].set_title('Eroded Mask')\n",
        "\n",
        "# Dilated mask\n",
        "axs[2].imshow(dilated_mask, cmap='gray')\n",
        "axs[2].set_title('Dilated Mask')\n",
        "\n",
        "# Remove the axes\n",
        "for ax in axs:\n",
        "    ax.axis('off')\n",
        "\n",
        "# Adjust the spacing between subplots\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ljT5xSO2impq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Connected Component Analysis: Connected component analysis is useful for identifying and labeling separate regions or objects within the segmented mask. It helps in distinguishing between different objects and can be used for further analysis or classification. OpenCV's cv2.connectedComponents() function can be used to perform connected component analysis."
      ],
      "metadata": {
        "id": "ym6lPQC9aITE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the segmented mask\n",
        "mask = cv2.imread('segmented_mask.png', 0)  # Assuming the mask is a grayscale image\n",
        "\n",
        "# Perform connected component analysis\n",
        "num_labels, labels = cv2.connectedComponents(mask)\n",
        "\n",
        "# Display the original mask and the labeled regions\n",
        "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
        "\n",
        "# Original mask\n",
        "axs[0].imshow(mask, cmap='gray')\n",
        "axs[0].set_title('Original Mask')\n",
        "\n",
        "# Labeled regions\n",
        "axs[1].imshow(labels, cmap='nipy_spectral')\n",
        "axs[1].set_title('Labeled Regions')\n",
        "\n",
        "# Remove the axes\n",
        "for ax in axs:\n",
        "    ax.axis('off')\n",
        "\n",
        "# Adjust the spacing between subplots\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "sxdHcM2Nimm3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this updated example, the code uses the cv2.connectedComponents() function from OpenCV to perform connected component analysis on the segmented mask. The resulting labels are then displayed using Matplotlib.\n",
        "\n",
        "Make sure you have the segmented mask image (e.g., 'segmented_mask.png') in the same directory as the script or modify the file path accordingly.\n",
        "\n",
        "The first subplot displays the original mask, and the second subplot displays the labeled regions using the nipy_spectral colormap for visual distinction between different labels.\n",
        "\n",
        "Note that cv2.connectedComponents() assigns different labels to each separate region in the mask, where the background is labeled as 0 and each region is assigned a unique label.\n"
      ],
      "metadata": {
        "id": "vHRQc0rAaZS7"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RXQe0Vfnar26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DE8oIb45arz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Contour Refinement: Contour refinement techniques, such as contour approximation or contour smoothing, can be applied to the segmented regions. These techniques help in reducing jaggedness or irregularities in the contours and improve the overall shape representation. OpenCV's cv2.approxPolyDP() and cv2.drawContours() functions are useful for contour refinement."
      ],
      "metadata": {
        "id": "n5mHJKmbaguh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the segmented mask\n",
        "mask = cv2.imread('segmented_mask.png', 0)  # Assuming the mask is a grayscale image\n",
        "\n",
        "# Find contours in the mask\n",
        "contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "# Create an empty mask for contour refinement\n",
        "refined_mask = np.zeros_like(mask)\n",
        "\n",
        "# Iterate over each contour\n",
        "for contour in contours:\n",
        "    # Perform contour approximation\n",
        "    epsilon = 0.03 * cv2.arcLength(contour, True)\n",
        "    approx = cv2.approxPolyDP(contour, epsilon, True)\n",
        "\n",
        "    # Draw the refined contour on the mask\n",
        "    cv2.drawContours(refined_mask, [approx], 0, (255), thickness=cv2.FILLED)\n",
        "\n",
        "# Display the original mask and the refined contour mask\n",
        "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
        "\n",
        "# Original mask\n",
        "axs[0].imshow(mask, cmap='gray')\n",
        "axs[0].set_title('Original Mask')\n",
        "\n",
        "# Refined contour mask\n",
        "axs[1].imshow(refined_mask, cmap='gray')\n",
        "axs[1].set_title('Refined Contour Mask')\n",
        "\n",
        "# Remove the axes\n",
        "for ax in axs:\n",
        "    ax.axis('off')\n",
        "\n",
        "# Adjust the spacing between subplots\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "M9bAVCNJimhz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B5_BkSVHauXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Histogram Equalization: Histogram equalization is a technique used to enhance the contrast and improve the overall appearance of an image. It redistributes the pixel intensities in such a way that the histogram becomes more evenly distributed. In Python, you can use the cv2.equalizeHist() function from OpenCV or the skimage.exposure.equalize_hist() function from scikit-image to perform histogram equalization."
      ],
      "metadata": {
        "id": "z2VW9gbT0f85"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the image\n",
        "image = cv2.imread('image.jpg')\n",
        "\n",
        "# Convert the image to YUV color space\n",
        "image_yuv = cv2.cvtColor(image, cv2.COLOR_BGR2YUV)\n",
        "\n",
        "# Apply histogram equalization to the Y channel\n",
        "image_yuv[:,:,0] = cv2.equalizeHist(image_yuv[:,:,0])\n",
        "\n",
        "# Convert the image back to BGR color space\n",
        "equalized_image = cv2.cvtColor(image_yuv, cv2.COLOR_YUV2BGR)\n",
        "\n",
        "# Display the original and equalized images\n",
        "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
        "\n",
        "# Original image\n",
        "axs[0].imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "axs[0].set_title('Original Image')\n",
        "\n",
        "# Equalized image\n",
        "axs[1].imshow(cv2.cvtColor(equalized_image, cv2.COLOR_BGR2RGB))\n",
        "axs[1].set_title('Equalized Image')\n",
        "\n",
        "# Remove the axes\n",
        "for ax in axs:\n",
        "    ax.axis('off')\n",
        "\n",
        "# Adjust the spacing between subplots\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ZB6qnTJrauUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Contrast Enhancement: Contrast enhancement techniques aim to increase the difference between the bright and dark areas of an image, resulting in a more visually pleasing and vibrant image. Common methods include histogram stretching, gamma correction, and adaptive contrast enhancement. OpenCV and scikit-image provide functions such as cv2.normalize() and skimage.exposure.adjust_gamma() for contrast enhancement."
      ],
      "metadata": {
        "id": "6HqUcXw00h4i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage import exposure\n",
        "\n",
        "# Load the image\n",
        "image = cv2.imread('image.jpg')\n",
        "\n",
        "# Convert the image to RGB color space\n",
        "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Perform histogram stretching\n",
        "stretched_image = exposure.rescale_intensity(image_rgb)\n",
        "\n",
        "# Perform gamma correction\n",
        "gamma_corrected_image = exposure.adjust_gamma(image_rgb, gamma=1.5)\n",
        "\n",
        "# Display the original, stretched, and gamma-corrected images\n",
        "fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "# Original image\n",
        "axs[0].imshow(image_rgb)\n",
        "axs[0].set_title('Original Image')\n",
        "\n",
        "# Stretched image\n",
        "axs[1].imshow(stretched_image)\n",
        "axs[1].set_title('Histogram Stretched Image')\n",
        "\n",
        "# Gamma-corrected image\n",
        "axs[2].imshow(gamma_corrected_image)\n",
        "axs[2].set_title('Gamma Corrected Image')\n",
        "\n",
        "# Remove the axes\n",
        "for ax in axs:\n",
        "    ax.axis('off')\n",
        "\n",
        "# Adjust the spacing between subplots\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "0iozn2MkauOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sharpening: Sharpening techniques are used to enhance the edges and fine details in an image, resulting in a clearer and more defined appearance. One popular method is the unsharp masking technique, which involves subtracting a blurred version of the image from the original image. OpenCV provides functions like cv2.filter2D() for sharpening an image using various kernel filters."
      ],
      "metadata": {
        "id": "5Jqz92aS01ii"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the image\n",
        "image = cv2.imread('image.jpg')\n",
        "\n",
        "# Convert the image to RGB color space\n",
        "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Convert the image to float32 for calculations\n",
        "image_float = image_rgb.astype(np.float32) / 255.0\n",
        "\n",
        "# Apply Gaussian blur to create the blurred version of the image\n",
        "blurred_image = cv2.GaussianBlur(image_float, (5, 5), 0)\n",
        "\n",
        "# Calculate the difference between the original and blurred images\n",
        "sharp_mask = cv2.subtract(image_float, blurred_image)\n",
        "\n",
        "# Enhance the edges by scaling the sharp mask\n",
        "sharp_mask = cv2.multiply(sharp_mask, 2.0)\n",
        "\n",
        "# Add the sharp mask to the original image\n",
        "sharpened_image = cv2.add(image_float, sharp_mask)\n",
        "\n",
        "# Convert the sharpened image back to uint8 format\n",
        "sharpened_image = (sharpened_image * 255.0).clip(0, 255).astype(np.uint8)\n",
        "\n",
        "# Display the original and sharpened images\n",
        "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
        "\n",
        "# Original image\n",
        "axs[0].imshow(image_rgb)\n",
        "axs[0].set_title('Original Image')\n",
        "\n",
        "# Sharpened image\n",
        "axs[1].imshow(sharpened_image)\n",
        "axs[1].set_title('Sharpened Image')\n",
        "\n",
        "# Remove the axes\n",
        "for ax in axs:\n",
        "    ax.axis('off')\n",
        "\n",
        "# Adjust the spacing between subplots\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "EDnh1txkauLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Noise Reduction: Image noise can degrade the quality of an image. Applying noise reduction techniques, such as median filtering, Gaussian filtering, or bilateral filtering, can help to reduce the noise and improve the image quality. OpenCV provides functions like cv2.medianBlur() and cv2.bilateralFilter() for noise reduction."
      ],
      "metadata": {
        "id": "oY4lJae71vYC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the image\n",
        "image = cv2.imread('image.jpg')\n",
        "\n",
        "# Convert the image to RGB color space\n",
        "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Apply median filtering\n",
        "median_filtered_image = cv2.medianBlur(image_rgb, 5)\n",
        "\n",
        "# Apply bilateral filtering\n",
        "bilateral_filtered_image = cv2.bilateralFilter(image_rgb, 9, 75, 75)\n",
        "\n",
        "# Display the original and filtered images\n",
        "fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "# Original image\n",
        "axs[0].imshow(image_rgb)\n",
        "axs[0].set_title('Original Image')\n",
        "\n",
        "# Median filtered image\n",
        "axs[1].imshow(median_filtered_image)\n",
        "axs[1].set_title('Median Filtered Image')\n",
        "\n",
        "# Bilateral filtered image\n",
        "axs[2].imshow(bilateral_filtered_image)\n",
        "axs[2].set_title('Bilateral Filtered Image')\n",
        "\n",
        "# Remove the axes\n",
        "for ax in axs:\n",
        "    ax.axis('off')\n",
        "\n",
        "# Adjust the spacing between subplots\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "qxeIAlvKauIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Super-Resolution: Super-resolution techniques aim to enhance the resolution and details of low-resolution images. These methods involve generating a high-resolution image from one or more low-resolution images. Deep learning-based approaches, such as the use of convolutional neural networks (CNNs), have shown promising results in super-resolution. Packages like TensorFlow or PyTorch can be used for implementing super-resolution models."
      ],
      "metadata": {
        "id": "PB_2IY8t1ymM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.transforms import ToTensor\n",
        "from torchvision.models import vgg19\n",
        "from torchvision.models.utils import load_state_dict_from_url\n",
        "\n",
        "# Define the EDSR model architecture\n",
        "class EDSR(nn.Module):\n",
        "    def __init__(self, scale_factor):\n",
        "        super(EDSR, self).__init__()\n",
        "        self.scale_factor = scale_factor\n",
        "        self.conv1 = nn.Conv2d(3, 256, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv3 = nn.Conv2d(256, 3 * scale_factor ** 2, kernel_size=3, stride=1, padding=1)\n",
        "        self.pixel_shuffle = nn.PixelShuffle(scale_factor)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = nn.functional.relu(self.conv1(x))\n",
        "        out = nn.functional.relu(self.conv2(out))\n",
        "        out = self.pixel_shuffle(self.conv3(out))\n",
        "        return out\n",
        "\n",
        "# Load the pre-trained weights for EDSR\n",
        "state_dict = load_state_dict_from_url(\"https://github.com/Saafke/EDSR_Tensorflow/blob/master/models/edsr_x4.pb?raw=true\")\n",
        "edsr = EDSR(scale_factor=4)\n",
        "edsr.load_state_dict(state_dict)\n",
        "\n",
        "# Load the low-resolution image\n",
        "image_lr = cv2.imread('low_res_image.jpg')\n",
        "\n",
        "# Convert the low-resolution image to a PyTorch tensor\n",
        "image_lr_tensor = ToTensor()(cv2.cvtColor(image_lr, cv2.COLOR_BGR2RGB)).unsqueeze(0)\n",
        "\n",
        "# Upscale the low-resolution image using the EDSR model\n",
        "with torch.no_grad():\n",
        "    image_sr_tensor = edsr(image_lr_tensor)\n",
        "\n",
        "# Convert the super-resolved image tensor to a numpy array\n",
        "image_sr = (image_sr_tensor.squeeze().numpy().transpose(1, 2, 0) * 255).clip(0, 255).astype(np.uint8)\n",
        "\n",
        "# Display the low-resolution and super-resolved images\n",
        "cv2.imshow('Low-Resolution Image', image_lr)\n",
        "cv2.imshow('Super-Resolved Image', image_sr)\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()\n"
      ],
      "metadata": {
        "id": "jLvybFT_2M2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ohDXEjDH2R_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9ZrZHm_h2MzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VX91dNhw2Mwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dG4yPeiW2Mtp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GQSkj-mi2Mqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hocr-m6N2Mny"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}